# Cloud :- gives you access to virtual machine where you can run your server. 
            We can't use local server worldwide, because of local machines don't have public IP
            So, EC2 gives you public Ip

            imp note:-
            Public IP always changes on stop and start ec2 server
            to avoid that use elastic IP 

# EC2 servers
    EC2 stands for Elastic compute version 2
    1) Elastic:- Can increase/decrease size of machine
    2) compute:- it's a machine


# USE CASES
1) rent server: EC2 server
    steps:-
        1. give name to your application
        2. select machine type eg. Ubuntu, Windows, MacOs, linux etc.
        3. Select type of instance eg. Micro ( have less pricing) -> small -> 2xl etc.
            pricing will increase based on type of instance
        4. Create new key pair 
            It gives you remotely access to AWS instance through aws CLI
            it will download .pem file which used in assess of aws instance

        # Security groups section
        SSH:- secure shell protocol through you can assess virtual machine

        by default PORTS of http and https
        1) http:- 80 ( you can access application through http://632.342.3.4) it assumes port listen at PORT 80
            that why we change PORT of server eg. if we listen on 3000 - you can assess through http://632.342.3.4:300
        2) https:- 443

        once instance in running state it will give you public Ip and public IPv4 dns  

        5. Click on launch instance:- it will launch the instance on AWS cloud

        don't forget to stop instance when not in use, otherwise it will increase cost/ bill of instance.
        select instance and click on stop instance option.

        connect to aws instance remotely:-
        1) go to .pem file location 
        2) run command:- ssh -i .\harkirat-class-password.pem windows@<publicIP>

        If you gets permission denied error run command
        chmod 700 <.pem file location> 
        700 convert in binary eg. -rwx------@ rwx === 111 in binary and rest of ----- === 00
        used to restrict access an give access.

        terminal commands
        1) exit --> to exit from instance
        2) git clone https://<repo address>
        3) ask for username and password
            type username 
            and for password generate  personal token form developer setting in github settings
        after that CLI will get git access

        # if don't have internet access
        run command vi /etc/resolv.conf file
        and insert 
        nameserver 8.8.8.8 
        and save file and exit

        install nodejs in ec2 instance

        https://medium.com/@rajani103/deploying-nodejs-app-on-aws-ec2-instance-step-by-step-1b00f807cdce

        with Installing Node Using the Node Version Manager
        curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh
       *** this will show script present into it

        curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash
       *** this will run script and install nvm on ec2

        install nvm --> nvm install <version>

        to open any PORT in ec2 ( open your application port for ec2)
        add it into inbound port list

        steps:
        1. Click on security 
        2. click on your security group in
        3. click on edit inbound rules
        4. click on add rule
        5. select custom tcp ---> give PORT --> select anywhere from IPV4
        6. select custom tcp ---> give PORT --> select anywhere from IPV6

        nodejs process should not listen on PORT 80 OR 443

        to access server 
        type public IP with port in address bar eg. 13.201.94.183:8080 --> where you access your deployed remote server

----- run server in background 
        Run service Even if terminal was close
        install pm2 ( Process manager )
        command:- npm i -g pm2 
        pm2 start index.js   it will run service forever
        pm2 logs ---> show logs of that service 
        exit --> to exit from machine/terminal

        we can run multiple ports on single node process


# FrontEnd deployment
    1) Database is conventionally used to add searchable elements ( eg. string etc )
     other than that content called object ( images, mp4 ect)
     To store object there are object stores ( eg. aws s3 Simple storage 3) 


    ** Request of OBJECT STORES goes through CDN

    CDNs ( content delivery networks )
    As the name suggests, itâ€™s an optimal way for you to deliver content (mp4 files, .jpgs and even HTML/CSS/JS files) to your users.
    
    Cloudfront distribution is used to create CDN 
    Create CDN in AWS

    For frontends, mp4 files, images, Object stores  + CDNs are a better approach.

    steps:- 
    1) Create s3 bucket
    2) create Cloudfront distribution
    3) give source ( s3 URL ) of s3 to Cloudfront
    4) Use Cloudfront ID AS CDN and You can access s3 bucket files

    CDNs are fast because of they cached content in nearest server after getting request

    We Not use CDN for backend, because we can't cached backend data ( cached data not live )

    #Edge network
    Deploying backend server on various places and when frontend makes request it will get response from nearest edge network ( backed server )

    Deploying On s3
    This approach will not work for frameworks that use "Server side rendering" (like Next.js)
    This will work for basic React apps, HTML/CSS/JS apps

    serve local react project
    steps:- 
        1.npm run build ---> Converts react into html css code
        2.npm i -g serve ---> Install serve globally
        3.cd ./build
        4. run serve command in build directory

        uploading on s3
        steps:- 
        1. Create s3 bucket with default configuration
        2. Create build of react-application
        3. Upload dist/build files and folders on s3 bucket
        4. go to Cloudfront dashboard
        5. Create Cloudfront distribution
        6. Original domain:- you can add anything ( eg. https://google.com OR your domain)
            Origin path:- if your files in dist or any your folder the give that folder name 
        because s3 bucket is not public set origin access as Origin access control settings ( recommanded )
        don't select public 
        7. Create new OAC origin access
        this will give you OAC file that you have to put in in s3 to give access of s3 to Cloudfront
        8. select enable security protection --> it will protect from security attacks
        imp:-
        9. Set default Root Object --> entry of your application eg. index.html ( if your using Cloudfront from website hosting)
        in case of mp4 Or images upload then not required

        access images or videos from Cloudfront 
        <Cloudfront_name>/<file_name>  --> it will give you that file

        10. click on create Cloudfront distribution
        11. After successfully creation it will give you one message to update s3 bucket policy and give you policy configuration to add in s3 policy permission
        12. go to s3 --> permissions --> click on edit bucket policy
            --> copy policy given by Cloudfront 
            --> paste it in policy editor in edit bucket policy
        13. click on save changes --> it will update s3 bucket policy
        14. After successfully deployed on Cloudfront use "Distribution domain name" to access your website in browser.

        # Assign you domain to Cloudfront web-application
        steps:- 
        1. In Alternate domain name --> enter you custom domain name
        
        Add error page in Cloudfront website:
        steps:- 
        1) select Error Pages
        2) Click on create custom error page
        3) Select error code
        4) Customized error response --> Yes
        5) give file path of error page --> You can add index.js
        6) Select status code 200

        Don't forgot to invalid cached
        steps:-
        1. Click on create invalidation
        2. Add Object path as --> /*
         it will clear cache  



# connect to EC2 instance with putty by aws .ppk file
steps:- 
1) open putty app
2) put EC2 public IP  in Host name ( or IP ) field.
3) select connection type SSH from connection type
4) from left menu click on connection 
    ---> SSH 
        ---> Auth 
            ---> 
                Credentials 
                    ---> upload .ppk file in Private key file for authentication input  
                        ---> then click on open and enter ec2 instance username in login as 
        then you'll connect to ec2 instance.


Crete bucket for image upload
1) create bucket 
2) Go to inside bucket
3) click on bucket permission
4) edit bucket policy
5)  a. click on add statement button
    b.attach s3 policy ---> search s3 in edit statement 
    c. search putObject --> check that option
    d. search getObject --> check that option
    e. set principle --> *
6) Add new resource
    a. select service -> s3
    b. resource type -> object
    c. resource ARN --> arn:aws:s3:::{BucketName}/{ObjectName} --> replace your bucketname and put objectname as "*" ( this allows all objects)
7) Click on save changes
8) set CORS to bucket 
    a. click on Cross-origin resource sharing (CORS) edit button
    b. https://docs.aws.amazon.com/AmazonS3/latest/userguide/ManageCorsUsing.html add example 2 json into cors
    c. make all origin open --> "AllowedOrigins": ["*"]
    d. "ExposeHeaders": []
    e. then save changes

9) Create user to get assess_key and secret_key
10) attach policy to user --> give aws features access --> give amazon s3 full access 
11) go to security Credentials --> 
    a. create access key
    b. select Application running on aws compute service
    c. click on next
    d. click on create assess_key ( don't give assess_key and secret_key to anyone)

    